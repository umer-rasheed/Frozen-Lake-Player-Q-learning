{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen-Lake-Player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Envrironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD ENVIRONMENT\n",
    "# Import Modules\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Environment\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Note that there is a Wind which occasionally blows the Agent onto a State they didn’t choose.\n",
      "Hence, Perfect Performance every single time is impossible.\n",
      "However, Learning to avoid the Holes and reach the Goal is certainly still doable.\n",
      "The Reward at every Step is 0, except for entering the Goal, which provides a Reward of 1.\n"
     ]
    }
   ],
   "source": [
    "# LOAD ENVIRONMENT\n",
    "# Load Environment from OpenAI Gym and Render \n",
    "print('Load Environment')\n",
    "Env=gym.make('FrozenLake-v0')\n",
    "Env.render()\n",
    "print('')\n",
    "print('Note that there is a Wind which occasionally blows the Agent onto a State they didn’t choose.')\n",
    "print('Hence, Perfect Performance every single time is impossible.')\n",
    "print('However, Learning to avoid the Holes and reach the Goal is certainly still doable.')\n",
    "print('The Reward at every Step is 0, except for entering the Goal, which provides a Reward of 1.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Action Size ', 4)\n",
      "('State Size ', 16)\n"
     ]
    }
   ],
   "source": [
    "# LOAD ENVIRONMENT\n",
    "# Explore Environment\n",
    "ActionSize=Env.action_space.n\n",
    "print(\"Action Size \",ActionSize)\n",
    "StateSize=Env.observation_space.n\n",
    "print(\"State Size \",StateSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZATION\n",
    "# Initialize Q-Table\n",
    "QTable=np.zeros((StateSize,ActionSize))\n",
    "print(QTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-NETWORK\n",
    "# Reset TensorFlow Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Initialize the Feed-Forward Network\n",
    "Inputs=tf.placeholder(shape=[1,StateSize],dtype=tf.float32)\n",
    "Weights=tf.Variable(tf.random_uniform([StateSize,ActionSize],0,0.01))\n",
    "QOut=tf.matmul(Inputs,Weights)\n",
    "Predict=tf.argmax(QOut,1)\n",
    "\n",
    "# Loss Computation (Sum of Squares Difference between the Target and Prediction Q-Values)\n",
    "NextQ=tf.placeholder(shape=[1,ActionSize],dtype=tf.float32)\n",
    "Loss=tf.reduce_sum(tf.square(NextQ-QOut))\n",
    "Trainer=tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "UpdateModel=Trainer.minimize(Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "# Initializa Variable\n",
    "Init=tf.initialize_all_variables()\n",
    "\n",
    "# Set Learning Parameters\n",
    "Y=0.99\n",
    "E=0.10\n",
    "NumEpisodes=20000\n",
    "\n",
    "# Create Lists to contain Total Rewards and Steps per Episode\n",
    "JList=[]\n",
    "RList=[]\n",
    "\n",
    "# Run Session\n",
    "with tf.Session() as Sess:\n",
    "    Sess.run(Init)\n",
    "    for i in range(NumEpisodes):\n",
    "        # Reset Environment\n",
    "        State=Env.reset()\n",
    "        RewardsAll=0\n",
    "        D=False\n",
    "        Step=0\n",
    "        MaxSteps=99\n",
    "        # The Q-Network\n",
    "        while Step<MaxSteps:\n",
    "            Step+=1\n",
    "            # Choose an Action (A) in Current World State (S)\n",
    "            Action,AllQ=Sess.run([Predict,QOut],feed_dict={Inputs:np.identity(StateSize)[State:State+1]})\n",
    "            \n",
    "            # Check if this number is Greater than Epsilon, Then Exploitation (Take the Biggest Q-Value for this State)\n",
    "            if np.random.rand(1)<E:\n",
    "                Action[0]=Env.action_space.sample()\n",
    "                \n",
    "            # Take the Action (A) and Observe the Outcome State(S') and Reward (R)\n",
    "            NewState,Reward,D,_= Env.step(Action[0])\n",
    "            \n",
    "            # Obtain the Q' Values by Feeding the New State through our Network\n",
    "            Q1=Sess.run(QOut,feed_dict={Inputs:np.identity(StateSize)[NewState:NewState+1]})\n",
    "            \n",
    "            # Obtain Maximum Q' and Set our Target Value for Chosen Action\n",
    "            MaxQ1=np.max(Q1)\n",
    "            TargetQ=AllQ\n",
    "            TargetQ[0,Action[0]]=Reward+Y*MaxQ1\n",
    "            \n",
    "            # Train the Network using Target and Predicted Q-Values\n",
    "            _,NewWeights=Sess.run([UpdateModel,Weights],feed_dict={Inputs:np.identity(StateSize)[State:State+1],NextQ:TargetQ})\n",
    "            \n",
    "            # Update Rewards\n",
    "            RewardsAll+=Reward\n",
    "            \n",
    "            # Update State\n",
    "            State=NewState\n",
    "            if D==True:\n",
    "                # Change Epsilon\n",
    "                E=1./((i/50)+10)\n",
    "                break\n",
    "        JList.append(Step)\n",
    "        RList.append(RewardsAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of Succesful Episodes: 0.652%\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "# Training Error\n",
    "print \"Percent of Succesful Episodes: \" + str(sum(RList)/NumEpisodes) + \"%\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
